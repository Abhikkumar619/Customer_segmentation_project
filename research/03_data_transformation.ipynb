{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.Customer_segementation.constant import *\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig: \n",
    "    root_dir: Path\n",
    "    data_dir: Path\n",
    "    train_data__scaled_path: Path\n",
    "    test_data_scaled_path: Path\n",
    "    test_size: int\n",
    "    random_state: int\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    preprocess_data_path: Path\n",
    "    preprocess_path: Path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Customer_segementation.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configurationManager: \n",
    "    def __init__(self,config_file_path=CONFIG_FILE_PATH,\n",
    "                 schema_file_path=SCHEMA_FILE_PATH,\n",
    "                 params_file_path=PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(config_file_path)\n",
    "        self.schema=read_yaml(schema_file_path)\n",
    "        self.params=read_yaml(params_file_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self)->DataTransformationConfig:\n",
    "       config=self.config.data_transformation\n",
    "       parmas=self.params.data_transformation\n",
    "       create_directories([config.root_dir])\n",
    "\n",
    "       get_transformation_config=DataTransformationConfig(\n",
    "           root_dir=config.root_dir,\n",
    "           data_dir=config.data_dir,\n",
    "           train_data__scaled_path=config.train_data_scaled_path,\n",
    "           test_data_scaled_path=config.test_data_scaled_path,\n",
    "           test_size=parmas.test_size,\n",
    "           random_state=parmas.random_state,\n",
    "           train_data_path=config.train_data_path,\n",
    "           test_data_path=config.test_data_path,\n",
    "           preprocess_data_path=config.preprocess_Data,\n",
    "           preprocess_path=config.preprocessor_path\n",
    "           \n",
    "           )\n",
    "       return get_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.Customer_segementation.logger import logger\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Customer_segementation.utils.common import save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation: \n",
    "    def __init__(self, config=DataTransformationConfig) -> None:\n",
    "        self.config=config\n",
    "\n",
    "        Gender=['Male', 'Female']\n",
    "        Ever_Married=['No', 'Yes']\n",
    "        Graduated=['No', 'Yes']\n",
    "        Profession=['Healthcare', 'Engineer', 'Lawyer', 'Entertainment', 'Artist','Executive', 'Doctor', 'Homemaker', 'Marketing']\n",
    "        Spending_Score=['Low', 'Average', 'High']\n",
    "\n",
    "\n",
    "    def train_test_data_split(self, data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        method name: train test data split\n",
    "        description: It divide data into training data and testing dataset.\n",
    "        \"\"\"\n",
    "        # data.drop('Var_1', inplace=True, axis=1)\n",
    "        train_data, test_data=train_test_split(data, test_size=self.config.test_size, \n",
    "                                               random_state=self.config.random_state)\n",
    "        \n",
    "        # Save train data and test data.\n",
    "\n",
    "        train_data.to_csv(self.config.train_data_path, index=False, header=True)\n",
    "        test_data.to_csv(self.config.test_data_path, index=False, header=True)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def get_data_preprocessing(self, cat_data:list, num_data:list):\n",
    "        \"\"\"\n",
    "        method_name: get data preprocessing\n",
    "        decription: In this method feature engineering pipeline created that handle the missing vlaue and scale data.\n",
    "        \"\"\"\n",
    "        Gender=['Male', 'Female']\n",
    "        Ever_Married=['No', 'Yes']\n",
    "        Graduated=['No', 'Yes']\n",
    "        Profession=['Healthcare', 'Engineer', 'Lawyer', 'Entertainment', 'Artist','Executive', 'Doctor', 'Homemaker', 'Marketing']\n",
    "        Spending_Score=['Low', 'Average', 'High']\n",
    "\n",
    "        num_pipeline=Pipeline(\n",
    "            steps=[\n",
    "        (\"imputer\",SimpleImputer()),\n",
    "        (\"scaler\",StandardScaler())\n",
    "        ])\n",
    "        cat_pipeline=Pipeline([\n",
    "            ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot',OrdinalEncoder(categories=[Gender, Ever_Married, Graduated, Profession, Spending_Score]))])\n",
    "        \n",
    "        preprocessor=ColumnTransformer([\n",
    "            (\"num_pipeline\",num_pipeline,num_data),\n",
    "            (\"cat_pipeline\",cat_pipeline,cat_data)])\n",
    "        \n",
    "        return preprocessor\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def initate_data_transformation(self):\n",
    "        data=pd.read_csv(self.config.data_dir)\n",
    "        # logger.info(f\"Customer segementation data: {data.head()}\")\n",
    "        \n",
    "        # drop the Id columns from dataset\n",
    "        data.drop('ID', inplace=True, axis=1)\n",
    "\n",
    "        categorical_data=[feature for feature in data.columns if data[feature].dtypes=='O']\n",
    "        categorical_data=['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score']\n",
    "        numerical_data=[feature for feature in data.columns if data[feature].dtypes !='O']\n",
    "\n",
    "        logger.info(f\"categorical_data : {categorical_data}\")\n",
    "        logger.info(f\"Numerical data: {numerical_data}\")\n",
    "\n",
    "        preprocessing=self.get_data_preprocessing(cat_data=categorical_data, num_data=numerical_data)\n",
    "\n",
    "        # logger.info(f\"Preprocessing model created: {preprocessing}\")\n",
    "\n",
    "        preprocess_data=pd.DataFrame(preprocessing.fit_transform(data), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "        # logger.info(f\"Data Preprocess: {preprocess_data.head()}\")\n",
    "        # save_object(Path(self.config.preprocess_path), obj=preprocessing)\n",
    "        \n",
    "\n",
    "        preprocess_data.to_csv(self.config.preprocess_data_path, index=False, header=True)\n",
    "\n",
    "\n",
    "        self.train_test_data_split(preprocess_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-01 17:39:30,846 : INFO : common : Yaml file read config/config.yaml successfully]\n",
      "[2024-07-01 17:39:30,849 : INFO : common : Yaml file read schema.yaml successfully]\n",
      "[2024-07-01 17:39:30,851 : INFO : common : Yaml file read params.yaml successfully]\n",
      "[2024-07-01 17:39:30,851 : INFO : common : Directories created ['artifacts']]\n",
      "[2024-07-01 17:39:30,852 : INFO : common : Directories created ['artifacts/data_transformation']]\n",
      "[2024-07-01 17:39:30,864 : INFO : 223563250 : categorical_data : ['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score']]\n",
      "[2024-07-01 17:39:30,865 : INFO : 223563250 : Numerical data: ['Age', 'Work_Experience', 'Family_Size']]\n",
      "[2024-07-01 17:39:30,888 : INFO : common : Object save at: <_io.BufferedWriter name='artifacts/data_transformation/preprocessor.pkl'>]\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    configmanager=configurationManager()\n",
    "    transformation_config=configmanager.get_data_transformation_config()\n",
    "    # logger.info(f\"{transformation_config}\")\n",
    "    data_transformation=DataTransformation(transformation_config)\n",
    "    data_transformation.initate_data_transformation()\n",
    "\n",
    "except Exception as e: \n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
